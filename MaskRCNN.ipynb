{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "DfPPQ6ztJhv4"
      },
      "source": [
        "# Mask RCNN Model for Segmenting Breast Ultrasound Images in Tumour Detection\n",
        "\n",
        "The following Jupyter Notebook uses the PyTorch machine learning framework to train and test on a pre-trained Mask Region-based Convolutional Neural Network (Mask RCNN). Seen here: https://pytorch.org/vision/main/models/generated/torchvision.models.detection.maskrcnn_resnet50_fpn.html\n",
        "\n",
        "The dataset used consists of images from breast ultrasound scans that are categorized into two classes depending on the tumour (or lack thereof): normal, benign, and malignant.\n",
        "\n",
        "However, as you will notice during the data preparation segment, we will not be making use of the normal images. The model will be trained on benign and malignant classes only.\n",
        "\n",
        "Each ultrasound scan contains a mask image that segment where the tumour is.\n",
        "\n",
        "The goal of the model is, given an ultrasound image, be able to segment the location of the tumour and return a mask.\n",
        "\n",
        "The Notebook is divided as follows:\n",
        "\n",
        "* Data preparation\n",
        "\n",
        "* Creating the custom Dataset\n",
        "\n",
        "* Training and testing the model on a single sample\n",
        "\n",
        "* Instantiate our Dataset, DataLoader, hyperparameters, and model\n",
        "\n",
        "* Train the model\n",
        "\n",
        "* Evaluate and output performance\n",
        "\n",
        "* Save the model\n",
        "\n",
        "**Note**: This follows the same pattern and logic as the Penn-Fudan tutorial by PyTorch, seen here: https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1E8BBefDkfUh"
      },
      "source": [
        "## How to run\n",
        "\n",
        "1. Click on `Runtime` from the bar above.\n",
        "2. Click on `change runtime type`.\n",
        "3. Select `GPU` as your hardware accelerator and save.\n",
        "4. Connect to a runtime by clicking on `Connect` in the top right-hand side.\n",
        "5. Click on `Runtime` again and `Run all`."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Pmt84SwTRaFy"
      },
      "source": [
        "Importing all necessary libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mJSw7Z2PLVL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import numpy as np\n",
        "import sys\n",
        "import matplotlib.pyplot as plt \n",
        "import torch\n",
        "import torchvision\n",
        "import torch.utils.data\n",
        "import random\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from PIL import Image, ImageChops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h68aQFPWUYFm",
        "outputId": "8487362c-9503-4e75-e4c2-468e6b09cd1a"
      },
      "outputs": [],
      "source": [
        "%%shell\n",
        "\n",
        "# download TorchVision repo to use some files from references/detection\n",
        "git clone https://github.com/pytorch/vision.git\n",
        "cd vision\n",
        "git checkout v0.8.2\n",
        "\n",
        "cp references/detection/utils.py ../\n",
        "cp references/detection/transforms.py ../\n",
        "cp references/detection/coco_eval.py ../\n",
        "cp references/detection/engine.py ../\n",
        "cp references/detection/coco_utils.py ../"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yxEAgdZbOSVW",
        "outputId": "274283ab-5884-4546-b67f-26d6e9b97ba8"
      },
      "outputs": [],
      "source": [
        "# !pip uninstall torch -y\n",
        "# !pip uninstall torchtext -y\n",
        "# !pip uninstall torchdata -y\n",
        "# !pip uninstall torchaudio -y\n",
        "!pip install torch==1.12.0+cu113 torchvision==0.13.0+cu113 --extra-index-url https://download.pytorch.org/whl/cu113\n",
        "!pip install cython\n",
        "!pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B75QOJOJUoWh"
      },
      "outputs": [],
      "source": [
        "from engine import train_one_epoch, evaluate\n",
        "import utils\n",
        "import transforms as T"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5Sd4jlGp2eLm"
      },
      "source": [
        "## Data preperation"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXQRGXLnd6Ft"
      },
      "source": [
        "Download data zip file and create directories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "II9sQZhNI3bS",
        "outputId": "85026772-9f92-4b82-967e-7ccdbbb133fa"
      },
      "outputs": [],
      "source": [
        "!gdown 1LljpoDlVfLoowaG6qAq_rCzX7W6wVjql"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "geCmyHJyJDRS",
        "outputId": "f091e17f-72b7-4a49-a3ac-3da876013b9d"
      },
      "outputs": [],
      "source": [
        "%%shell\n",
        "unzip data.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42KKBHhFCxWn",
        "outputId": "9fb7c496-5553-4812-b4b8-0819c3fe060c"
      },
      "outputs": [],
      "source": [
        "%%shell\n",
        "mkdir Dataset/\n",
        "cd Dataset/\n",
        "mkdir UltrasoundImages\n",
        "mkdir MaskImages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wUKw1n689BcN"
      },
      "outputs": [],
      "source": [
        "# converts mask images to greyscale and sets the objects to either 0, 1, 2, or 3 depending on the number of objects present\n",
        "def img_to_index_array(path, index):\n",
        "    mask = Image.open(path).convert('L')\n",
        "    mask_asarray = np.asarray(mask)\n",
        "    mask_asarray[mask_asarray>0] = index\n",
        "\n",
        "    return mask_asarray"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "E-m8FwUSW7Zv"
      },
      "source": [
        "Creates a tuple combining each ultrasound image with it's corresponding mask image.\n",
        "\n",
        "Copies and renames the images into either the ultrasound folder or the mask folder.\n",
        "\n",
        "After doing so, the following structure will be created:\n",
        "\n",
        "```\n",
        "Dataset/\n",
        "  MaskImages/\n",
        "    Mask00000_mask.png\n",
        "    Mask00001_mask.png\n",
        "    Mask00002_mask.png\n",
        "    Mask00003_mask.png\n",
        "    ...\n",
        "  UltrasoundImages/\n",
        "    Image00000.png\n",
        "    Image00001.png\n",
        "    Image00002.png\n",
        "    Image00003.png\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h0ohbGJygfcB"
      },
      "outputs": [],
      "source": [
        "path_benign = 'ultrasound/benign/'\n",
        "path_malignant = 'ultrasound/malignant/'\n",
        "path = [path_benign, path_malignant]\n",
        "\n",
        "\n",
        "data = []\n",
        "for item in path:\n",
        "  img_dir = os.listdir(item)\n",
        "  for image in img_dir:\n",
        "    if 'mask' not in image:\n",
        "      ultrasound_image = image \n",
        "      mask_image = image.replace(')', ')_mask')\n",
        "      if mask_image in img_dir:\n",
        "        img_msk_tuple = [item + ultrasound_image, item + mask_image]\n",
        "      for mask_id in ['1', '2']:\n",
        "        mask_image_extra = image.replace(')', ')_mask_'+ mask_id)\n",
        "        if mask_image_extra in img_dir:\n",
        "          img_msk_tuple.append(item + mask_image_extra)\n",
        "      data.append(tuple(img_msk_tuple))\n",
        "\n",
        "# ultrasound images\n",
        "i = 0 \n",
        "for item in data:\n",
        "  if i < 10:\n",
        "    shutil.copy(item[0], os.path.join('Dataset/UltrasoundImages/', 'Image000'+str(i)+'.png'))\n",
        "  elif i <100 and i >= 10:\n",
        "    shutil.copy(item[0], os.path.join('Dataset/UltrasoundImages/', 'Image00'+str(i)+'.png'))\n",
        "  else:\n",
        "    shutil.copy(item[0], os.path.join('Dataset/UltrasoundImages/', 'Image0'+str(i)+'.png'))\n",
        "  i = i+1\n",
        "\n",
        "\n",
        "# mask images\n",
        "index = [0,1,2,3] # 0 is a placeholder\n",
        "i = 0\n",
        "for item in data:\n",
        "  if len(item) - 1 == 1:  # first index is an ultrasound image, therefore -1\n",
        "    mask_asarray = img_to_index_array(item[1], index[1])\n",
        "  elif len(item) - 1 == 2:\n",
        "    mask_asarray_1 = img_to_index_array(item[1], index[1])\n",
        "    mask_asarray_2 = img_to_index_array(item[2], index[2])\n",
        "    mask_asarray = mask_asarray_1 +  mask_asarray_2\n",
        "  else:\n",
        "    mask_asarray_1 = img_to_index_array(item[1], index[1])\n",
        "    mask_asarray_2 = img_to_index_array(item[2], index[2])\n",
        "    mask_asarray_3 = img_to_index_array(item[3], index[3]) \n",
        "    mask_asarray = mask_asarray_1 +  mask_asarray_2 + mask_asarray_3\n",
        "\n",
        "  mask_composite = Image.fromarray(mask_asarray) \n",
        "\n",
        "  if i < 10:\n",
        "    mask_composite.save(os.path.join('Dataset/MaskImages/Mask000'+str(i)+'.png'))\n",
        "  elif i <100 and i >= 10:   \n",
        "    mask_composite.save(os.path.join('Dataset/MaskImages/Mask00'+str(i)+'.png'))\n",
        "  else:\n",
        "    mask_composite.save(os.path.join('Dataset/MaskImages/Mask0'+str(i)+'.png'))\n",
        "\n",
        "  i = i+1\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-FiyHFNSYwHn"
      },
      "source": [
        "Here is one example of an image in the dataset, with its corresponding  segmentation mask.\n",
        "\n",
        "Then showcasing the same mask with a palette."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 834
        },
        "id": "mJxfWhsZ_ei6",
        "outputId": "e860bcd0-0ff5-4487-9f5d-f793c5717498"
      },
      "outputs": [],
      "source": [
        "ultrasound_image = Image.open('/content/Dataset/UltrasoundImages/Image0001.png')\n",
        "mask_image = Image.open('/content/Dataset/MaskImages/Mask0001.png')\n",
        "\n",
        "mask_array = np.asarray(mask_image)\n",
        "mask_array[mask_array >= 1] = 255\n",
        "mask_image = Image.fromarray(mask_array)\n",
        "\n",
        "ultrasound_image = ultrasound_image.resize((500, 400), Image.ANTIALIAS)\n",
        "mask_image = mask_image.resize((500, 400), Image.ANTIALIAS)\n",
        "\n",
        "ultrasound_image.show(), mask_image.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        },
        "id": "p42Dc2UB_jQS",
        "outputId": "9e5933ac-2e79-49ee-eeb0-3966c2d7b2f7"
      },
      "outputs": [],
      "source": [
        "mask = Image.open('/content/Dataset/MaskImages/Mask0001.png')\n",
        "mask_withPalette = mask.convert('P')\n",
        "mask_withPalette.putpalette([\n",
        "    0, 0, 0, # black background\n",
        "    255, 0, 0, # index 1 is red\n",
        "    255, 255, 0, # index 2 is yellow\n",
        "    255, 153, 0, # index 3 is orange\n",
        "])\n",
        "mask_withPalette"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "C9Ee5NV54Dmj"
      },
      "source": [
        "## Creating our Dataset class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mTgWtixZTs3X"
      },
      "outputs": [],
      "source": [
        "class UltrasoundDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root, transforms=None):\n",
        "        self.root = root\n",
        "        self.transforms = transforms\n",
        "        # load all image files, sorting them to ensure all ultrasound images align with its corresponding mask image\n",
        "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"UltrasoundImages\"))))\n",
        "        self.masks = list(sorted(os.listdir(os.path.join(root, \"MaskImages\"))))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # load images and masks\n",
        "        img_path = os.path.join(self.root, \"UltrasoundImages\", self.imgs[idx])\n",
        "        mask_path = os.path.join(self.root, \"MaskImages\", self.masks[idx])\n",
        "        \n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        # mask is not converted to RGB because each color corresponds to a different instance with 0 being the background\n",
        "        mask = Image.open(mask_path)\n",
        "\n",
        "        mask = np.array(mask)\n",
        "        # instances are encoded as different colors\n",
        "        obj_ids = np.unique(mask)\n",
        "        # first id is the background, so remove it\n",
        "        obj_ids = obj_ids[1:]\n",
        "\n",
        "        # split the color-encoded mask into a set of binary masks\n",
        "        masks = mask == obj_ids[:, None, None]\n",
        "\n",
        "        # get bounding box coordinates for each mask\n",
        "        num_objs = len(obj_ids)\n",
        "        boxes = []\n",
        "        for i in range(num_objs):\n",
        "            pos = np.where(masks[i])\n",
        "            xmin = np.min(pos[1])\n",
        "            xmax = np.max(pos[1])\n",
        "            ymin = np.min(pos[0])\n",
        "            ymax = np.max(pos[0])\n",
        "            boxes.append([xmin, ymin, xmax, ymax])\n",
        "\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        # there is only one class\n",
        "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
        "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
        "\n",
        "        image_id = torch.tensor([idx])\n",
        "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = labels\n",
        "        target[\"masks\"] = masks\n",
        "        target[\"image_id\"] = image_id\n",
        "        target[\"area\"] = area\n",
        "        target[\"iscrowd\"] = iscrowd\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            img, target = self.transforms(img, target)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "J6f3ZOTJ4Km9"
      },
      "source": [
        "Let's take a look at the output of our Dataset class.\n",
        "\n",
        "A `PIL` image and a target dictionary containing the required data for the model to process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEARO4B_ye0s",
        "outputId": "bd6dd585-8815-4436-f7b5-395963a70e80"
      },
      "outputs": [],
      "source": [
        "dataset = UltrasoundDataset('Dataset/')\n",
        "dataset[0]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "AAy77d9ocOIJ"
      },
      "source": [
        "As we are using a pre-trained model, we will need to finetune it to our specific dataset. \n",
        "\n",
        "The following function will take the number of classes we have and return a instance segmentation Mask RCNN model to fit with our data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YjNHjVMOyYlH"
      },
      "outputs": [],
      "source": [
        "def get_instance_segmentation_model(num_classes):\n",
        "  \n",
        "    # load an instance segmentation model pre-trained on COCO\n",
        "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
        "\n",
        "    # get the number of input features for the classifier\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    # replace the pre-trained head with a new one\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "    # now get the number of input features for the mask classifier\n",
        "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
        "    hidden_layer = 256\n",
        "    # and replace the mask predictor with a new one\n",
        "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
        "                                                       hidden_layer,\n",
        "                                                       num_classes)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-WXLwePV5ieP"
      },
      "source": [
        "## Transforms and training/testing the model on a single sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l79ivkwKy357"
      },
      "outputs": [],
      "source": [
        "def get_transform(train):\n",
        "    transforms = []\n",
        "    # converts the image, a PIL image, into a PyTorch Tensor\n",
        "    transforms.append(T.ToTensor())\n",
        "    if train:\n",
        "        # during training, randomly flip the training images and ground truth for data augmentation\n",
        "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
        "    return T.Compose(transforms)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ovjC81FZfWat"
      },
      "source": [
        "We will take a look at what a single forward pass on an image looks like before iterating over our Dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-NFR--2fXV3",
        "outputId": "cacd17c1-410e-462c-a6b1-d941f12a5157"
      },
      "outputs": [],
      "source": [
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "dataset = UltrasoundDataset('Dataset', get_transform(train=True))\n",
        "data_loader = torch.utils.data.DataLoader(\n",
        "    dataset, batch_size=2, shuffle=True, num_workers=0,\n",
        "    collate_fn=utils.collate_fn\n",
        ")\n",
        "\n",
        "# training\n",
        "images, targets = next(iter(data_loader))\n",
        "images = list(image for image in images)\n",
        "targets = [{k: v for k, v in t.items()} for t in targets]\n",
        "output = model(images, targets)   # returns losses and detections\n",
        "\n",
        "# inference/testing\n",
        "model.eval()\n",
        "x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n",
        "predictions = model(x) # returns predictions"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3YFJGJxk6XEs"
      },
      "source": [
        "## Defining our hyperparameters and training our model\n",
        "\n",
        "After setting up our data, Dataset, and transforms, it's time to apply everything together."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5dGaIezze3y"
      },
      "outputs": [],
      "source": [
        "# use our Dataset and defined transformations\n",
        "dataset = UltrasoundDataset('Dataset', get_transform(train=True))\n",
        "dataset_test = UltrasoundDataset('Dataset', get_transform(train=False))\n",
        "\n",
        "# split the Dataset in train and test set\n",
        "indices = torch.randperm(len(dataset)).tolist()\n",
        "dataset = torch.utils.data.Subset(dataset, indices[:-50])\n",
        "dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n",
        "\n",
        "# define training and testing DataLoader\n",
        "data_loader = torch.utils.data.DataLoader(\n",
        "    dataset, batch_size=2, shuffle=True, num_workers=0,\n",
        "    collate_fn=utils.collate_fn)\n",
        "\n",
        "data_loader_test = torch.utils.data.DataLoader(\n",
        "    dataset_test, batch_size=1, shuffle=False, num_workers=0,\n",
        "    collate_fn=utils.collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AURxMoliyDLs",
        "outputId": "2342ec5d-3bd6-44b4-dc1b-7bb4bfe9a7e3"
      },
      "outputs": [],
      "source": [
        "# check on the number of examples in each Dataset\n",
        "len(dataset), len(dataset_test)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "L5yvZUprj4ZN"
      },
      "source": [
        "Insantiate the model along with optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zoenkCj18C4h",
        "outputId": "c65d1692-a3fe-41a4-b7a6-127fc3759c46"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "# our dataset has two classes only - background and tumour\n",
        "num_classes = 2\n",
        "\n",
        "# create the model using our helper function\n",
        "model = get_instance_segmentation_model(num_classes).to(device)\n",
        "\n",
        "# construct an optimizer\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr=0.005,\n",
        "                            momentum=0.9, weight_decay=0.0005)\n",
        "\n",
        "# and a learning rate scheduler which decreases the learning rate by 10x every 3 epochs\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
        "                                               step_size=3,\n",
        "                                               gamma=0.1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "XAd56lt4kDxc"
      },
      "source": [
        "We can now train the model.\n",
        "\n",
        "We will be using 10 epochs and the `train_one_epoch` helper function from PyTorch's torchvision package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "at-h4OWK0aoc",
        "outputId": "b8477f3f-e32b-434c-d28b-03e9c5ae49e9"
      },
      "outputs": [],
      "source": [
        "# let's train it for 10 epochs\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  \n",
        "    # train for one epoch, printing every 10 iterations\n",
        "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
        "\n",
        "    # update the learning rate\n",
        "    lr_scheduler.step()\n",
        "\n",
        "    # evaluate on the test dataset\n",
        "    evaluate(model, data_loader_test, device=device)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6mYGFLxkO8F"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "Afer completing training, we can take a look at what the model outputs when passing it a test image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHwIdxH76uPj"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "# pick one image from the test set\n",
        "img, mask = dataset_test[30]\n",
        "\n",
        "# put the model in evaluation mode\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    prediction = model([img.to(device)])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "DmN602iKsuey"
      },
      "source": [
        "Taking a look at the `prediction`, we can see it stores a list of dictionaries containing the specific information we defined back in the Dataset class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "OixU0GgV5A4t",
        "outputId": "2082c9f6-c73a-4206-ef20-80d01dbd7c38"
      },
      "outputs": [],
      "source": [
        "plt.imshow(mask['masks'].squeeze(dim=0).cpu().numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lkmb3qUu6zw3",
        "outputId": "08042b89-6d03-439a-bc5f-b0bd8a123ef5"
      },
      "outputs": [],
      "source": [
        "prediction"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "RwT21rzotFbH"
      },
      "source": [
        "We can now examine the test image we gave our model and its predicted mask."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "bpqN9t1u7B2J",
        "outputId": "b81fd2bd-a3eb-4937-d104-8f21c20b4433"
      },
      "outputs": [],
      "source": [
        "Image.fromarray(img.mul(255).permute(1, 2, 0).byte().numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "5v5S3bm07SO1",
        "outputId": "0d3c46c5-93de-4e36-a8c0-20a9c6ad7140"
      },
      "outputs": [],
      "source": [
        "Image.fromarray(prediction[0]['masks'][0, 0].mul(255).byte().cpu().numpy())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5GEgnKm6x3PS"
      },
      "source": [
        "The following function will take in our model and produce `n` predictions and display them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKetRiWHx_W-"
      },
      "outputs": [],
      "source": [
        "def display_predictions(model, n=10):\n",
        "\n",
        "  ultrasound_files = []\n",
        "  ground_truth_files = []\n",
        "  pred_files = []\n",
        "\n",
        "  for i in range(n):\n",
        "\n",
        "    r = random.randint(0, len(dataset_test)-1)\n",
        "    img, mask = dataset_test[r]\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      prediction = model([img.to(device)])\n",
        "\n",
        "    ultrasound_files.append(img)\n",
        "    ground_truth_files.append(mask)\n",
        "    pred_files.append(prediction)\n",
        "\n",
        "  fig, ax = plt.subplots(n, 3, figsize=(15, 30))\n",
        "\n",
        "  for idx, ground_truth_image in enumerate(ultrasound_files):\n",
        "\n",
        "    if idx == 0:\n",
        "      ax[idx, 0].set_title(\"Ultrasound image\")\n",
        "      ax[idx, 1].set_title(\"Ground truth mask\")\n",
        "      ax[idx, 2].set_title(\"Predicted mask\")\n",
        "\n",
        "    # fetch the ultrasound image for the corresponding index above\n",
        "    ultrasound_image = ultrasound_files[idx]\n",
        "    ultrasound_image = ultrasound_image.permute(1, 2, 0)\n",
        "\n",
        "    # fetch the ground truth image for the corresponding index above\n",
        "    ground_truth_image = ground_truth_files[idx]\n",
        "    ground_truth_image = Image.fromarray(ground_truth_image['masks'].squeeze(dim=0).cpu().numpy())\n",
        "\n",
        "    # fetch the pred image for the corresponding index above\n",
        "    pred_image = Image.fromarray(pred_files[idx][0]['masks'][0, 0].mul(255).byte().cpu().numpy())\n",
        "\n",
        "    ax[idx, 0].imshow(ultrasound_image)\n",
        "    ax[idx, 0].axis(False)\n",
        "\n",
        "    ax[idx, 1].imshow(ground_truth_image)\n",
        "    ax[idx, 1].axis(False)\n",
        "\n",
        "    ax[idx, 2].imshow(pred_image)\n",
        "    ax[idx, 2].axis(False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UvaiUUs26nR3",
        "outputId": "b1dfe88f-4421-415f-905f-2daffae10f9b"
      },
      "outputs": [],
      "source": [
        "display_predictions(model)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "voS_h7OVxMS4"
      },
      "source": [
        "## Saving our model for later use if needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYRj0zOqpqqy"
      },
      "outputs": [],
      "source": [
        "def save_checkpoint(state, filename=\"MyMaskRCNN_Model.pth\"):\n",
        "    torch.save(state, filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28GVtVaipsLG"
      },
      "outputs": [],
      "source": [
        "state = {\n",
        "    \"state_dict\": model.state_dict(),\n",
        "    \"optimizer\":optimizer.state_dict(),\n",
        "}\n",
        "save_checkpoint(state)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "DfPPQ6ztJhv4"
      },
      "source": [
        "# U-Net Model for Segmenting Breast Ultrasound Images in Tumour Detection\n",
        "\n",
        "The following Jupyter Notebook uses the PyTorch machine learning framework to train and test on a U-Net model built from the ground up.\n",
        "\n",
        "The dataset used consists of images from breast ultrasound scans that are categorized into two classes depending on the tumour (or lack thereof): normal, benign, and malignant.\n",
        "\n",
        "However, as you will notice during the data preparation segment, we will not be making use of the normal images. The model will be trained on benign and malignant classes only.\n",
        "\n",
        "Each ultrasound scan contains a mask image that segment where the tumour is.\n",
        "\n",
        "The goal of the model is, given an ultrasound image, be able to segment the location of the tumour and return a mask.\n",
        "\n",
        "The Notebook is divided as follows:\n",
        "\n",
        "* Data preparation\n",
        "\n",
        "* Creating the custom Dataset and U-Net model\n",
        "\n",
        "* Create helper functions such as saving/loading the model, fetch DataLoader's and so on. \n",
        "\n",
        "* Set the hyperparameters, transforms, and training loop function.\n",
        "\n",
        "* Instantiate and train the model with our `main()` function.\n",
        "\n",
        "* Evaluate and output performance\n",
        "\n",
        "* Save the model\n",
        "\n",
        "**Note**: This follows the same pattern and logic as the repository by Aladdin Persson, seen here: https://github.com/aladdinpersson/Machine-Learning-Collection/tree/master/ML/Pytorch/image_segmentation/semantic_segmentation_unet\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1E8BBefDkfUh"
      },
      "source": [
        "## How to run\n",
        "\n",
        "1. Click on `Runtime` from the bar above.\n",
        "2. Click on `change runtime type`.\n",
        "3. Select `GPU` as your hardware accelerator and save.\n",
        "4. Connect to a runtime by clicking on `Connect` in the top right-hand side.\n",
        "5. Click on `Runtime` again and `Run all`."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QUi_ikiTOXTC"
      },
      "source": [
        "## If you are using a pre-trained model please [go here](#scrollTo=ANNsD_rGKtfF&line=7&uniqifier=1)."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "25MmDeuMf_bp"
      },
      "source": [
        "## Data preparation"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Pmt84SwTRaFy"
      },
      "source": [
        "Importing all necessary libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sPbKd3t5M2M8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "import torch.optim as optim\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch.utils.data import Dataset, DataLoader, sampler\n",
        "import torchvision.transforms.functional as TF\n",
        "import pandas as pd\n",
        "import torchvision\n",
        "import time\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import zipfile\n",
        "import requests\n",
        "import os\n",
        "import shutil\n",
        "import random\n",
        "import glob\n",
        "import pandas as pd\n",
        "from typing import Dict, List\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import gc\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MhCCggMw4jty"
      },
      "outputs": [],
      "source": [
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXQRGXLnd6Ft"
      },
      "source": [
        "Download data zip file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VqjPn4Q1c8Kk",
        "outputId": "b5624713-dca2-4ece-d1d7-f894cf6bf9fa"
      },
      "outputs": [],
      "source": [
        "# gdown allows downloading large files from Google Drive by passing id\n",
        "!gdown 1LljpoDlVfLoowaG6qAq_rCzX7W6wVjql"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tQH8Nu2ffR-p"
      },
      "source": [
        "Create `data` directory and unzip contents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wHkUunH4YAZF"
      },
      "outputs": [],
      "source": [
        "data_path = 'data'\n",
        "\n",
        "if os.path.isdir(data_path):\n",
        "  print(\"The directory already exists.\")\n",
        "else:\n",
        "    with zipfile.ZipFile(\"data.zip\", \"r\") as zip_ref:\n",
        "      zip_ref.extractall(\"data\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6C5P19ga1VAl"
      },
      "source": [
        "The following three cells perform a similar data preperation logic.\n",
        "\n",
        "The first is for normal scans and masks, the second for benign, and third for malignant.\n",
        "\n",
        "* Sets paths for training and testing folders.\n",
        "* Create ratio that will be used to split training and testing data.\n",
        "* Fetches all the images for the specified class and shuffles them.\n",
        "* Splits the images into train and test sets.\n",
        "* Create the directories where we will be storing them.\n",
        "* Move the train and test data to the train and test directories, respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbCtKB7HPBgX",
        "outputId": "5b8dea79-9012-41d7-b956-c10384f8c68b"
      },
      "outputs": [],
      "source": [
        "path_to_normal = 'data/ultrasound/normal'\n",
        "\n",
        "if os.path.exists(path_to_normal):\n",
        "  \n",
        "  # set the path to the output folders for the train and test data\n",
        "  train_normal = 'data/train/normal'\n",
        "  test_normal = 'data/test/normal'\n",
        "\n",
        "  # set the train/test split ratio\n",
        "  train_ratio = 0.70\n",
        "\n",
        "  # get a list of all image file names in the folder\n",
        "  normal_image_filenames = os.listdir(path_to_normal)\n",
        "\n",
        "  data = []\n",
        "\n",
        "  for image in normal_image_filenames:\n",
        "    if 'mask' not in image:\n",
        "      ultrasound_image = image\n",
        "      mask_image = image.replace(')', ')_mask')\n",
        "      data.append((ultrasound_image, mask_image))\n",
        "\n",
        "  random.shuffle(data)\n",
        "\n",
        "  # split the image filenames into train and test sets\n",
        "  split_index = int(train_ratio * len(data))\n",
        "  train_image_filenames = data[:split_index]\n",
        "  test_image_filenames = data[split_index:]\n",
        "\n",
        "  # create the train and test output folders if they don't already exist\n",
        "  if not os.path.exists(train_normal):\n",
        "    os.makedirs(train_normal)\n",
        "  if not os.path.exists(test_normal):\n",
        "    os.makedirs(test_normal)\n",
        "\n",
        "  # copy the train images to the train output folder\n",
        "  for ultrasound_image, mask_image in train_image_filenames:\n",
        "    shutil.copy(os.path.join(path_to_normal, ultrasound_image), os.path.join(train_normal, ultrasound_image)) # copy the ultrasound image to the training normal directory\n",
        "    shutil.copy(os.path.join(path_to_normal, mask_image), os.path.join(train_normal, mask_image)) # copy the mask image to the training normal directory\n",
        "\n",
        "\n",
        "  # copy the test images to the test output folder\n",
        "  for  ultrasound_image, mask_image in test_image_filenames:\n",
        "    shutil.copy(os.path.join(path_to_normal, ultrasound_image), os.path.join(test_normal, ultrasound_image)) # copy the ultrasound image to the testing normal directory\n",
        "    shutil.copy(os.path.join(path_to_normal, mask_image), os.path.join(test_normal, mask_image)) # copy the mask image to the testing normal directory\n",
        "\n",
        "\n",
        "  print(f\"Finished splitting {len(normal_image_filenames)} images into train and test sets.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSCrvEnwUZ67",
        "outputId": "3d568f5b-b70c-4c34-abb9-0e45c0552566"
      },
      "outputs": [],
      "source": [
        "path_to_benign = 'data/ultrasound/benign'\n",
        "\n",
        "if os.path.exists(path_to_benign):\n",
        "  \n",
        "  # set the path to the output folders for the train and test data\n",
        "  train_benign = 'data/train/benign'\n",
        "  test_benign = 'data/test/benign'\n",
        "\n",
        "  # set the train/test split ratio\n",
        "  train_ratio = 0.70\n",
        "\n",
        "  # get a list of all image file names in the folder\n",
        "  benign_image_filenames = os.listdir(path_to_benign)\n",
        "\n",
        "  data = []\n",
        "\n",
        "  for image in benign_image_filenames:\n",
        "    if 'mask' not in image:\n",
        "      ultrasound_image = image\n",
        "      mask_image = image.replace(')', ')_mask')\n",
        "      data.append((ultrasound_image, mask_image))\n",
        "\n",
        "  random.shuffle(data)\n",
        "\n",
        "  # split the image filenames into train and test sets\n",
        "  split_index = int(train_ratio * len(data))\n",
        "  train_image_filenames = data[:split_index]\n",
        "  test_image_filenames = data[split_index:]\n",
        "\n",
        "  # create the train and test output folders if they don't already exist\n",
        "  if not os.path.exists(train_benign):\n",
        "    os.makedirs(train_benign)\n",
        "  if not os.path.exists(test_benign):\n",
        "    os.makedirs(test_benign)\n",
        "\n",
        "  # copy the train images to the train output folder\n",
        "  for ultrasound_image, mask_image in train_image_filenames:\n",
        "    shutil.copy(os.path.join(path_to_benign, ultrasound_image), os.path.join(train_benign, ultrasound_image)) # copy the ultrasound image to the training benign directory\n",
        "    shutil.copy(os.path.join(path_to_benign, mask_image), os.path.join(train_benign, mask_image)) # copy the mask image to the training benign directory\n",
        "\n",
        "\n",
        "  # copy the test images to the test output folder\n",
        "  for  ultrasound_image, mask_image in test_image_filenames:\n",
        "    shutil.copy(os.path.join(path_to_benign, ultrasound_image), os.path.join(test_benign, ultrasound_image)) # copy the ultrasound image to the testing benign directory\n",
        "    shutil.copy(os.path.join(path_to_benign, mask_image), os.path.join(test_benign, mask_image)) # copy the mask image to the testing benign directory\n",
        "\n",
        "\n",
        "  print(f\"Finished splitting {len(benign_image_filenames)} images into train and test sets.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Gw_wZbEU1lS",
        "outputId": "c74f8e69-d525-40d9-e25c-54094f2b6d15"
      },
      "outputs": [],
      "source": [
        "path_to_malignant = 'data/ultrasound/malignant'\n",
        "\n",
        "if os.path.exists(path_to_malignant):\n",
        "  \n",
        "  # set the path to the output folders for the train and test data\n",
        "  train_malignant = 'data/train/malignant'\n",
        "  test_malignant = 'data/test/malignant'\n",
        "\n",
        "  # set the train/test split ratio\n",
        "  train_ratio = 0.70\n",
        "\n",
        "  # get a list of all image file names in the folder\n",
        "  malignant_image_filenames = os.listdir(path_to_malignant)\n",
        "\n",
        "  data = []\n",
        "\n",
        "  for image in malignant_image_filenames:\n",
        "    if 'mask' not in image:\n",
        "      ultrasound_image = image\n",
        "      mask_image = image.replace(')', ')_mask')\n",
        "      data.append((ultrasound_image, mask_image))\n",
        "\n",
        "  random.shuffle(data)\n",
        "\n",
        "  # split the image filenames into train and test sets\n",
        "  split_index = int(train_ratio * len(data))\n",
        "  train_image_filenames = data[:split_index]\n",
        "  test_image_filenames = data[split_index:]\n",
        "\n",
        "  # create the train and test output folders if they don't already exist\n",
        "  if not os.path.exists(train_malignant):\n",
        "    os.makedirs(train_malignant)\n",
        "  if not os.path.exists(test_malignant):\n",
        "    os.makedirs(test_malignant)\n",
        "\n",
        "  # copy the train images to the train output folder\n",
        "  for ultrasound_image, mask_image in train_image_filenames:\n",
        "    shutil.copy(os.path.join(path_to_malignant, ultrasound_image), os.path.join(train_malignant, ultrasound_image)) # copy the ultrasound image to the training malignant directory\n",
        "    shutil.copy(os.path.join(path_to_malignant, mask_image), os.path.join(train_malignant, mask_image)) # copy the mask image to the training malignant directory\n",
        "\n",
        "\n",
        "  # copy the test images to the test output folder\n",
        "  for  ultrasound_image, mask_image in test_image_filenames:\n",
        "    shutil.copy(os.path.join(path_to_malignant, ultrasound_image), os.path.join(test_malignant, ultrasound_image)) # copy the ultrasound image to the testing malignant directory\n",
        "    shutil.copy(os.path.join(path_to_malignant, mask_image), os.path.join(test_malignant, mask_image)) # copy the mask image to the testing malignant directory\n",
        "\n",
        "\n",
        "  print(f\"Finished splitting {len(malignant_image_filenames)} images into train and test sets.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ced7ssmfAAAr"
      },
      "outputs": [],
      "source": [
        "# moving ultrasound dir out of 'data/' path\n",
        "check_dir = 'ultrasound'\n",
        "\n",
        "if os.path.isdir(check_dir):\n",
        "  print('Dataset already moved.')\n",
        "else:\n",
        "  root = '/content'\n",
        "  dataset = 'data/ultrasound'\n",
        "  shutil.move(dataset, root)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "H8wBpC8omYg8"
      },
      "source": [
        "Loop through both directories we created above, and remove either the ultrasound image or mask image.\n",
        "\n",
        "Creating a path to the training images and masks, and testing images and masks.\n",
        "\n",
        "Note that we also exclude all normal images as we will be focusing on benign and malignant alone."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBBVVzKVlIl8"
      },
      "outputs": [],
      "source": [
        "path_to_images, path_to_masks = [], []\n",
        "\n",
        "for directory_path, directory_names, file_names in os.walk('data/train'):\n",
        "    # loop through all image files in the current directory\n",
        "    for file_name in file_names:\n",
        "        if \"mask\" not in file_name and \"normal\" not in file_name:\n",
        "            # add the file if it does not contains \"mask\" in its filename\n",
        "            path_to_images.append(os.path.join(directory_path, file_name))\n",
        "\n",
        "for directory_path, directory_names, file_names in os.walk('data/train'):\n",
        "    # loop through all image files in the current directory\n",
        "    for file_name in file_names:\n",
        "        if \"mask\" in file_name and \"normal\" not in file_name:\n",
        "            # add the file if it does contains \"mask\" in its filename\n",
        "            path_to_masks.append(os.path.join(directory_path, file_name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YgAvmbf_F9-p"
      },
      "outputs": [],
      "source": [
        "path_to_images_test, path_to_masks_test = [], []\n",
        "\n",
        "for directory_path, directory_names, file_names in os.walk('data/test'):\n",
        "    # loop through all image files in the current directory\n",
        "    for file_name in file_names:\n",
        "        if \"mask\" not in file_name and \"normal\" not in file_name:\n",
        "            # add the file if it does not contains \"mask\" in its filename\n",
        "            path_to_images_test.append(os.path.join(directory_path, file_name))\n",
        "\n",
        "for directory_path, directory_names, file_names in os.walk('data/test'):\n",
        "    # loop through all image files in the current directory\n",
        "    for file_name in file_names:\n",
        "        if \"mask\" in file_name and \"normal\" not in file_name:\n",
        "            # add the file if it does contains \"mask\" in its filename\n",
        "            path_to_masks_test.append(os.path.join(directory_path, file_name))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "C9Ee5NV54Dmj"
      },
      "source": [
        "## Creating our Dataset class and U-Net model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C4u8AuxEmCGb"
      },
      "outputs": [],
      "source": [
        "class UltrasoundDataset(Dataset):\n",
        "    def __init__(self, image_dir, mask_dir, transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.transform = transform\n",
        "        self.images = image_dir\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        img_path = self.images[index]\n",
        "        \n",
        "        # getting mask path\n",
        "        c = self.getClass(image_path=img_path)\n",
        "        number_of_data = int(''.join(filter(str.isdigit, img_path)))\n",
        "        result = [i for i in self.mask_dir if c in i and '(' + str(number_of_data) + ')' in i]\n",
        "        mask_path = result[0]\n",
        "\n",
        "        image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
        "        mask = np.array(Image.open(mask_path).convert(\"L\"), dtype=np.float32)\n",
        "        mask[mask == 255.0] = 1.0\n",
        "\n",
        "        if self.transform is not None:\n",
        "            augmentations = self.transform(image=image, mask=mask)\n",
        "            image = augmentations[\"image\"]\n",
        "            mask = augmentations[\"mask\"]\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "    def getClass(self, image_path):\n",
        "\n",
        "      if 'normal' in image_path:\n",
        "        c = 'normal'\n",
        "      elif 'benign' in image_path:\n",
        "        c = 'benign'\n",
        "      elif 'malignant' in image_path:\n",
        "        c = 'malignant'\n",
        "\n",
        "      return c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1kB7k7IHmdXK"
      },
      "outputs": [],
      "source": [
        "class DoubleConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(DoubleConv, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "class UNET(nn.Module):\n",
        "    def __init__(\n",
        "            self, in_channels=3, out_channels=1, features=[64, 128, 256, 512],\n",
        "    ):\n",
        "        super(UNET, self).__init__()\n",
        "        self.ups = nn.ModuleList()\n",
        "        self.downs = nn.ModuleList()\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Down part of UNET\n",
        "        for feature in features:\n",
        "            self.downs.append(DoubleConv(in_channels, feature))\n",
        "            in_channels = feature\n",
        "\n",
        "        # Up part of UNET\n",
        "        for feature in reversed(features):\n",
        "            self.ups.append(\n",
        "                nn.ConvTranspose2d(\n",
        "                    feature*2, feature, kernel_size=2, stride=2,\n",
        "                )\n",
        "            )\n",
        "            self.ups.append(DoubleConv(feature*2, feature))\n",
        "\n",
        "        self.bottleneck = DoubleConv(features[-1], features[-1]*2)\n",
        "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        skip_connections = []\n",
        "\n",
        "        for down in self.downs:\n",
        "            x = down(x)\n",
        "            skip_connections.append(x)\n",
        "            x = self.pool(x)\n",
        "\n",
        "        x = self.bottleneck(x)\n",
        "        skip_connections = skip_connections[::-1]\n",
        "\n",
        "        for idx in range(0, len(self.ups), 2):\n",
        "            x = self.ups[idx](x)\n",
        "            skip_connection = skip_connections[idx//2]\n",
        "\n",
        "            if x.shape != skip_connection.shape:\n",
        "                x = TF.resize(x, size=skip_connection.shape[2:])\n",
        "\n",
        "            concat_skip = torch.cat((skip_connection, x), dim=1)\n",
        "            x = self.ups[idx+1](concat_skip)\n",
        "\n",
        "        return self.final_conv(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlgQ5EuBLbaf",
        "outputId": "78feb3f5-e483-4b24-da6e-69fd191cfdc3"
      },
      "outputs": [],
      "source": [
        "# quick test on our model to check all is working well\n",
        "def test():\n",
        "    x = torch.randn((3, 1, 161, 161))\n",
        "    model = UNET(in_channels=1, out_channels=1)\n",
        "    preds = model(x)\n",
        "    assert preds.shape == x.shape\n",
        "\n",
        "test()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "t1sOBMiEIYXr"
      },
      "source": [
        "## Creating our helper functions\n",
        "\n",
        "These include:\n",
        "\n",
        "* `save_checkpoint` - Will save a copy of the model with all its weights and biases / state dict.\n",
        "\n",
        "* `load_checkpoint` - Loads a given model.\n",
        "\n",
        "* `get_loaders` - Creates the Dataset and DataLoader for our training set and testing/validation set.\n",
        "\n",
        "* `check_accuracy` - Tests on the current state of the model given a DataLoader and prints an accuracy metric and a Dice score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qWNIDvNPmZLN"
      },
      "outputs": [],
      "source": [
        "def save_checkpoint(state, filename=\"UNET_model.pth\"):\n",
        "    torch.save(state, filename)\n",
        "\n",
        "def load_checkpoint(checkpoint, model):\n",
        "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "\n",
        "def get_loaders(\n",
        "    train_dir,\n",
        "    train_maskdir,\n",
        "    val_dir,\n",
        "    val_maskdir,\n",
        "    batch_size,\n",
        "    train_transform,\n",
        "    val_transform,\n",
        "    num_workers=4,\n",
        "    pin_memory=True,\n",
        "):\n",
        "    train_ds = UltrasoundDataset(\n",
        "        image_dir=train_dir,\n",
        "        mask_dir=train_maskdir,\n",
        "        transform=train_transform,\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_ds,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=pin_memory,\n",
        "        shuffle=True,\n",
        "    )\n",
        "\n",
        "    val_ds = UltrasoundDataset(\n",
        "        image_dir=val_dir,\n",
        "        mask_dir=val_maskdir,\n",
        "        transform=val_transform,\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_ds,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=pin_memory,\n",
        "        shuffle=False,\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader\n",
        "\n",
        "def check_accuracy(loader, model, device=\"cuda\"):\n",
        "    num_correct = 0\n",
        "    num_pixels = 0\n",
        "    dice_score = 0\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x = x.to(device)\n",
        "            y = y.to(device).unsqueeze(1)\n",
        "            preds = torch.sigmoid(model(x))\n",
        "            preds = (preds > 0.5).float()\n",
        "            num_correct += (preds == y).sum()\n",
        "            num_pixels += torch.numel(preds)\n",
        "            dice_score += (2 * (preds * y).sum()) / ((preds + y).sum() + 1e-8)\n",
        "\n",
        "    print(f\"Got {num_correct}/{num_pixels} with acc {num_correct/num_pixels*100:.2f}\")\n",
        "    print(f\"Dice score: {dice_score/len(loader)}\")\n",
        "    model.train()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2EDLbuzSKrDm"
      },
      "source": [
        "## Setting our training loop, transforms, hyperparameters, and other variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oyUmxYo6mO2S"
      },
      "outputs": [],
      "source": [
        "LEARNING_RATE = 1e-4\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "BATCH_SIZE = 1\n",
        "NUM_EPOCHS = 20\n",
        "NUM_WORKERS = 2\n",
        "IMAGE_HEIGHT = 600 \n",
        "IMAGE_WIDTH = 700\n",
        "PIN_MEMORY = True\n",
        "LOAD_MODEL = False\n",
        "TRAIN_IMG_DIR = path_to_images\n",
        "TRAIN_MASK_DIR = path_to_masks\n",
        "VAL_IMG_DIR = path_to_images_test\n",
        "VAL_MASK_DIR = path_to_masks_test\n",
        "\n",
        "train_transform = A.Compose(\n",
        "        [\n",
        "            A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
        "            A.Rotate(limit=35, p=1.0),\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.VerticalFlip(p=0.1),\n",
        "            A.Normalize(\n",
        "                mean=[0.0, 0.0, 0.0],\n",
        "                std=[1.0, 1.0, 1.0],\n",
        "                max_pixel_value=255.0,\n",
        "            ),\n",
        "            ToTensorV2(),\n",
        "        ],\n",
        "    )\n",
        "\n",
        "val_transforms = A.Compose(\n",
        "    [\n",
        "        A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
        "        A.Normalize(\n",
        "            mean=[0.0, 0.0, 0.0],\n",
        "            std=[1.0, 1.0, 1.0],\n",
        "            max_pixel_value=255.0,\n",
        "        ),\n",
        "        ToTensorV2(),\n",
        "    ],\n",
        ")\n",
        "\n",
        "def train_fn(loader, model, optimizer, loss_fn, scaler):\n",
        "    # loop = tqdm(loader)\n",
        "\n",
        "    for batch_idx, (data, targets) in enumerate(loader):\n",
        "        data = data.to(device=DEVICE)\n",
        "        targets = targets.float().unsqueeze(1).to(device=DEVICE)\n",
        "\n",
        "        # forward pass\n",
        "        with torch.cuda.amp.autocast():\n",
        "            predictions = model(data)\n",
        "            loss = loss_fn(predictions, targets)\n",
        "\n",
        "        # backward\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        # update tqdm loop\n",
        "        # loop.set_postfix(loss=loss.item())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "w0qy__5hNPD_"
      },
      "source": [
        "## Train our model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "SBlvWDfwNbkw"
      },
      "source": [
        "`main()` function will bring together all the pieces.\n",
        "\n",
        "* Create model, loss function, optimizer, and scaler.\n",
        "\n",
        "* Set up our DataLoader's.\n",
        "\n",
        "* Train depending on `NUM_EPOCHS` set above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_JpaloTNN6n"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    \n",
        "    # instantiate our model, loss function, and optimizer\n",
        "    model = UNET(in_channels=3, out_channels=1).to(DEVICE)\n",
        "    loss_fn = nn.BCEWithLogitsLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "    train_loader, val_loader = get_loaders(\n",
        "        TRAIN_IMG_DIR,\n",
        "        TRAIN_MASK_DIR,\n",
        "        VAL_IMG_DIR,\n",
        "        VAL_MASK_DIR,\n",
        "        BATCH_SIZE,\n",
        "        train_transform,\n",
        "        val_transforms,\n",
        "        NUM_WORKERS,\n",
        "        PIN_MEMORY\n",
        "    )\n",
        "\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        train_fn(train_loader, model, optimizer, loss_fn, scaler)\n",
        "\n",
        "        # check accuracy\n",
        "        check_accuracy(val_loader, model, device=DEVICE)\n",
        "\n",
        "    # save model\n",
        "    checkpoint = {\n",
        "        \"state_dict\": model.state_dict(),\n",
        "        \"optimizer\": optimizer.state_dict(),\n",
        "    }\n",
        "    filename = \"UNET_model_\" + str(NUM_EPOCHS) + \"_epochs\" + \".pth\"\n",
        "    save_checkpoint(checkpoint, filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSvFy8bW3WtE",
        "outputId": "2ab06712-4292-463d-e50f-71b01a6c833c"
      },
      "outputs": [],
      "source": [
        "main()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "scX4uPXtN76y"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "We will create a `saved_images` directory to store our predictions along with their ground truths and ultrasound scans."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fv-p8s-PCq53"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists('saved_images'):\n",
        "  os.makedirs('saved_images')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "VbuaGJ0zG0tH"
      },
      "source": [
        "Create the validation DataLoader again"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQQeMnu6DHKp"
      },
      "outputs": [],
      "source": [
        "val_transforms = A.Compose(\n",
        "        [\n",
        "            A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
        "            A.Normalize(\n",
        "                mean=[0.0, 0.0, 0.0],\n",
        "                std=[1.0, 1.0, 1.0],\n",
        "                max_pixel_value=255.0,\n",
        "            ),\n",
        "            ToTensorV2(),\n",
        "        ],\n",
        "    )\n",
        "\n",
        "val_ds = UltrasoundDataset(\n",
        "        image_dir=VAL_IMG_DIR,\n",
        "        mask_dir=VAL_MASK_DIR,\n",
        "        transform=val_transforms,\n",
        "    )\n",
        "\n",
        "val_loader = DataLoader(\n",
        "        val_ds,\n",
        "        batch_size=1,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=PIN_MEMORY,\n",
        "        shuffle=False,\n",
        "    )"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "iznsVVqDOY8j"
      },
      "source": [
        "Create a new instance of the model and load the saved model's weights and biases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0395mqjJqXV"
      },
      "outputs": [],
      "source": [
        "model = UNET(in_channels=3, out_channels=1).to(DEVICE)\n",
        "filename = \"UNET_model_\" + str(NUM_EPOCHS) + \"_epochs\" + \".pth\"\n",
        "load_checkpoint(torch.load(filename), model)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "CleeGljlOg-9"
      },
      "source": [
        "The following function will take a DataLoader, a model, and a destination path and perform forward passes on the model.\n",
        "\n",
        "The feature (ultrasound image), the label (mask), and the output (predicted mask) are all stored in the given directory.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Xzg9A-rJ_yG"
      },
      "outputs": [],
      "source": [
        "def save_predictions_as_imgs(loader, model, folder=\"saved_images\", device=\"cuda\"):\n",
        "\n",
        "    model.eval()\n",
        "    for idx, (x, y) in enumerate(loader):\n",
        "        x = x.to(device=device)\n",
        "        with torch.no_grad():\n",
        "            preds = torch.sigmoid(model(x))\n",
        "            preds = (preds > 0.5).float()\n",
        "        torchvision.utils.save_image(x, f\"{folder}/ultrasound_{idx}.png\")\n",
        "        torchvision.utils.save_image(preds, f\"{folder}/pred_{idx}.png\")\n",
        "        torchvision.utils.save_image(y, f\"{folder}/mask_{idx}.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWLjvkZYD504"
      },
      "outputs": [],
      "source": [
        "save_predictions_as_imgs(val_loader, model, \"saved_images/\", DEVICE)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "nEXCtMqkPQtt"
      },
      "source": [
        "After saving all our predictions, the `display_predictions()` funtion will display the ultrasound images, ground truth masks, and the predicted segmentations from the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8x9IiEASVFwa"
      },
      "outputs": [],
      "source": [
        "def display_predictions(folder=\"saved_images/\", n=10):\n",
        "\n",
        "  ultrasound_files = []\n",
        "  ground_truth_files = []\n",
        "  pred_images = []\n",
        "\n",
        "  for files in os.listdir(folder):\n",
        "    # loop through all files in the directory\n",
        "    if \"mask\" in files:\n",
        "        # add the file if it does not contains \"mask\" in its filename\n",
        "        ground_truth_files.append(os.path.join(folder, files))\n",
        "    elif \"ultrasound\" in files:\n",
        "      # add the file if it contains \"ultrasound\" in its filename\n",
        "      ultrasound_files.append(os.path.join(folder, files))\n",
        "    else:\n",
        "      # else it's a prediction\n",
        "      pred_images.append(os.path.join(folder, files))\n",
        "  \n",
        "  random_ground_truth_files = random.sample(ground_truth_files, n)\n",
        "\n",
        "  fig, ax = plt.subplots(n, 3, figsize=(15, 30))\n",
        "\n",
        "  for idx, ground_truth_image in enumerate(random_ground_truth_files):\n",
        "\n",
        "    if idx == 0:\n",
        "      ax[idx, 0].set_title(\"Ultrasound image\")\n",
        "      ax[idx, 1].set_title(\"Ground truth mask\")\n",
        "      ax[idx, 2].set_title(\"Predicted mask\")\n",
        "\n",
        "    # fetch the index for the current example we're looking at\n",
        "    ground_truth_image_number = ground_truth_image.split('/')[1].split('_')[1]\n",
        "\n",
        "    # fetch the ultrasound image for the corresponding index above\n",
        "    ultrasound_image = [i for i in ultrasound_files if ground_truth_image_number in i]\n",
        "    ultrasound_image = ultrasound_image[0]\n",
        "\n",
        "    # fetch the pred image for the corresponding index above\n",
        "    pred_image = [i for i in pred_images if ground_truth_image_number in i]\n",
        "    pred_image = pred_image[0]\n",
        "\n",
        "    ultrasound_image = Image.open(ultrasound_image)\n",
        "    ground_truth_image = Image.open(ground_truth_image)\n",
        "    pred_image = Image.open(pred_image)\n",
        "\n",
        "    ax[idx, 0].imshow(ultrasound_image)\n",
        "    ax[idx, 0].axis(False)\n",
        "\n",
        "    ax[idx, 1].imshow(ground_truth_image)\n",
        "    ax[idx, 1].axis(False)\n",
        "\n",
        "    ax[idx, 2].imshow(pred_image)\n",
        "    ax[idx, 2].axis(False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rbRUXuubW301",
        "outputId": "32f764d1-eca1-419a-a950-f27178814e88"
      },
      "outputs": [],
      "source": [
        "display_predictions(n=10)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QiKhj-4G0UGN"
      },
      "source": [
        "## Using our model to predict on a custom image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9CNzkJ72EYd"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "# fetch custom image \n",
        "response = requests.get('https://www.caperay.com/blog/wp-content/uploads/2021/01/Mehta_Clinical_Imaging_2021_Fig_4a-300x267.jpg')\n",
        "img = Image.open(BytesIO(response.content))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDBf5UYbg2v3"
      },
      "outputs": [],
      "source": [
        "# converting the PIL image to a tensor\n",
        "tensor_uint8 = torchvision.transforms.functional.pil_to_tensor(img)\n",
        "tensor_uint8 = tensor_uint8[:3, :, :]\n",
        "\n",
        "# converting our image to torch.float32 AND dividing by 255 to make all our values range from 0 to 1\n",
        "custom_image = tensor_uint8.type(torch.float32) / 255."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lhdk_fPu7jpV",
        "outputId": "954ccaf5-ea7c-4ef3-bb6b-a65bbc53c709"
      },
      "outputs": [],
      "source": [
        "# resize image using transform\n",
        "custom_image_transform = transforms.Compose([\n",
        "    transforms.Resize(size=(IMAGE_HEIGHT, IMAGE_WIDTH))\n",
        "])\n",
        "\n",
        "custom_image_transformed = custom_image_transform(custom_image)\n",
        "\n",
        "print(f\"Original shape: {custom_image.shape}\")\n",
        "print(f\"Transformed shape: {custom_image_transformed.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbOP3Mm78qhF"
      },
      "outputs": [],
      "source": [
        "# add a batch size\n",
        "custom_image_transformed = custom_image_transformed.unsqueeze(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8JpfS5v83Z4"
      },
      "outputs": [],
      "source": [
        "# forward pass on model\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  custom_image_pred = torch.sigmoid(model(custom_image_transformed.to(DEVICE)))\n",
        "  custom_image_pred = (custom_image_pred > 0.5).float()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "rXZjYgFL9LGx",
        "outputId": "797807a0-c9af-4536-c109-ee8e6d7f1fea"
      },
      "outputs": [],
      "source": [
        "# display ultrasound scan with predicted segmented mask\n",
        "\n",
        "imagePIL = torchvision.transforms.ToPILImage()(custom_image_pred.squeeze(0))\n",
        "\n",
        "ultrasound_image = img\n",
        "mask_image = imagePIL\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(10, 10))\n",
        "\n",
        "ax[0].imshow(ultrasound_image)\n",
        "ax[0].axis(False)\n",
        "\n",
        "ax[1].imshow(imagePIL)\n",
        "ax[1].axis(False)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ANNsD_rGKtfF"
      },
      "source": [
        "## Using a pre-trained model\n",
        "\n",
        "If you would like to use a pre-trained model instead of training one, then please follow the steps:\n",
        "\n",
        "* Copy the model's `.pth` file to Google Colab's root files directory.\n",
        "\n",
        "* After it has finished uploading, run the [Data preparation](#scrollTo=25MmDeuMf_bp) code cells to import all required libraries and set up the data properly.\n",
        "\n",
        "* Then, go to the [model definition here](#scrollTo=1kB7k7IHmdXK&line=29&uniqifier=1) and run to set up the model.\n",
        "\n",
        "* Afterwards, set the `LOAD_MODEL` bool below to true and run the code cell to load the model.\n",
        "\n",
        "* When the model has finished loading, run the cells within the following code blocks:\n",
        "\n",
        "  * [Creating our Dataset class and U-Net model](#scrollTo=C9Ee5NV54Dmj)\n",
        "  * [Creating our helper functions](#scrollTo=t1sOBMiEIYXr)\n",
        "  * [Setting our training loop, transforms, hyperparameters, and other variables](#scrollTo=2EDLbuzSKrDm)\n",
        "  * [Evaluation](#scrollTo=scX4uPXtN76y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GpV3A2EQKxYo"
      },
      "outputs": [],
      "source": [
        "LOAD_MODEL = False\n",
        "\n",
        "def load_checkpoint(checkpoint, model):\n",
        "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "\n",
        "if LOAD_MODEL:\n",
        "  model = UNET(in_channels=3, out_channels=1).to(DEVICE)\n",
        "  filename = \"MyUNet_Model.pth\"\n",
        "  load_checkpoint(torch.load(filename), model)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
